{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences = [sent.string.strip() for sent in doc]\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "from multiprocessing import Lock\n",
    "from collections import deque\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "corpusName = \"test\"\n",
    "FLAGS_POS_TAGGING = 1\n",
    "input_path = \"../../data/\"+corpusName+\"/intermediate/segmentation.txt\"\n",
    "\n",
    "\n",
    "# text = u'The trie <phrase> data structure </phrase> has many properties which make it especially attractive for representing large files of data. These properties include fast retrieval time, quick unsuccessful search determination, and finding the longest match to a given identifier. The main drawback is the space requirement. In this paper the concept of trie compaction is formalized. An exact algorithm for optimal trie compaction and three algorithms for approximate trie compaction are given, and an analysis of the three algorithms is done. The analysis indicate that for actual tries, reductions of around 70 percent in the space required by the uncompacted trie can be expected. The quality of the compaction is shown to be insensitive to the number of nodes, while a more relevant parameter is the alphabet size of the key.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "start_phrase = [{ORTH: u'<phrase>', LEMMA: u'', POS: u'START_PHRASE', TAG: u'START_PHRASE'}]\n",
    "end_phrase = [{ORTH: u'</phrase>', LEMMA: u'', POS: u'END_PHRASE', TAG: u'END_PHRASE'}]\n",
    "\n",
    "nlp.tokenizer.add_special_case(u'<phrase>', start_phrase)\n",
    "nlp.tokenizer.add_special_case(u'</phrase>', end_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    ## add space before and after <phrase> tags\n",
    "    text = re.sub(r\"<phrase>\", \" <phrase> \", text)\n",
    "    text = re.sub(r\"</phrase>\", \" </phrase> \", text)\n",
    "    # text = re.sub(r\"<phrase>\", \" \", text)\n",
    "    # text = re.sub(r\"</phrase>\", \" \", text)\n",
    "    ## add space before and after special characters\n",
    "    text = re.sub(r\"([.,!:?()])\", r\" \\1 \", text)\n",
    "    ## replace multiple continuous whitespace with a single one\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def find(haystack, needle):\n",
    "    \"\"\"Return the index at which the sequence needle appears in the\n",
    "    sequence haystack, or -1 if it is not found, using the Boyer-\n",
    "    Moore-Horspool algorithm. The elements of needle and haystack must\n",
    "    be hashable.\n",
    "\n",
    "    >>> find([1, 1, 2], [1, 2])\n",
    "    1\n",
    "\n",
    "    \"\"\"\n",
    "    h = len(haystack)\n",
    "    n = len(needle)\n",
    "    skip = {needle[i]: n - i - 1 for i in range(n - 1)}\n",
    "    i = n - 1\n",
    "    while i < h:\n",
    "        for j in range(n):\n",
    "            if haystack[i - j] != needle[-j - 1]:\n",
    "                i += skip.get(haystack[i], n)\n",
    "                break\n",
    "        else:\n",
    "            return i - n + 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.span.Span' object has no attribute 'token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-2d9e215cfba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_one_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;31m# process_one_doc()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-2d9e215cfba8>\u001b[0m in \u001b[0;36mprocess_one_doc\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mp_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"->\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.span.Span' object has no attribute 'token'"
     ]
    }
   ],
   "source": [
    "# tt = \"A direct and unified approach is used to analyze the efficiency of batched searching of sequential and <phrase>tree-structured</phrase> files. The analysis is applicable to arbitrary search distributions, and closed-form expressions are obtained for the expected batched searching cost and savings. In particular, we consider a search distribution satisfying <phrase>Zipf's law</phrase> for sequential files and four types of uniform (random) search distribution for sequential and <phrase>tree-structured</phrase> files. These results unify and extend earlier research on batched searching and estimating block accesses for <phrase>database systems</phrase>.\"\n",
    "# tt = 'We construct a three-layer <phrase>feedforward network</phrase> (one hidden layer) which can solve the N-bit parity problem employing just two <phrase>hidden units</phrase>. We discuss the implications of employing problem constraints into <phrase>transfer functions</phrase> for general pattern classification problems.'\n",
    "# tt = 'A review of the application of the quad tesseral representation tosupport <phrase>spatial reasoning</phrase> is presented. The principal feature of therepresentation is that it linearises multi-dimensional space, while stillproviding for the description of <phrase>individual objects</phrase> within that space andthe relationships that may exist between those objects (in any directionand through any number of dimensions). In addition the representation issupported by an arithmetic which allows the manipulation (translation etc.)of <phrase>spatial objects</phrase>. Consequently, when incorporated into a spatialreasoning system, all necessary processing can be implemented as if in onlyone dimension. This offers two significant advantages over moreconventional multi-directional approaches to <phrase>spatial reasoning</phrase>. Firstly,many of the concerns associated with the exponential increase in the numberor relations that need to be considered (as the number of dimensions underconsideration increases) are no longer relevant. Secondly, the computationalcost of manipulating and comparing <phrase>spatial objects</phrase> remains static at itsone dimensional level, regardless of the number of dimensions underconsideration.'\n",
    "tt = 'Galileo, a programming language for <phrase>database applications</phrase>, is presented. Galileo is a <phrase>strongly-typed</phrase>, interactive <phrase>programming language</phrase> designed specifically to support semantic data model features (classification, aggregation, and specialization), as well as the abstraction mechanisms of modern <phrase>programming languages</phrase> (types, abstract types, and modularization). The main contributions of Galileo are (a) a flexible type system to model database structure and semantic <phrase>integrity constraints</phrase>; (b) the inclusion of type hierarchies to support the specialization abstraction mechanisms of semantic data models; (c) a modularization mechanism to structure data and operations into interrelated units (d) the integration of abstraction mechanisms into an expression-based language that allows interactive use of the database without resorting to a new stand-alone query language.Galileo will be used in the immediate future as a tool for <phrase>database design</phrase> and, in the long term, as a high-level interface for DBMSs.'\n",
    "def process_one_doc():\n",
    "    article = clean_text(tt)\n",
    "    articleId = 0\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    phrases = []\n",
    "    output_token_list = []\n",
    "    \n",
    "    # go over once \n",
    "    q = deque()\n",
    "    IN_PHRASE_FLAG = False\n",
    "    for token in article.split(\" \"):\n",
    "        if token == \"<phrase>\":\n",
    "            IN_PHRASE_FLAG = True            \n",
    "        elif token == \"</phrase>\":\n",
    "            current_phrase_list = []\n",
    "            while (len(q) != 0):\n",
    "                current_phrase_list.append(q.popleft())\n",
    "            phrases.append(\" \".join(current_phrase_list).lower())\n",
    "            IN_PHRASE_FLAG = False\n",
    "        else:\n",
    "            if IN_PHRASE_FLAG: # in the middle of a phrase, push the (word, pos) tuple\n",
    "                q.append(token)\n",
    "\n",
    "            ## put all the token information into the output fields\n",
    "            output_token_list.append(token)\n",
    "            \n",
    "    text = \" \".join(output_token_list)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "#         print(sent.start, sent.end)\n",
    "        sentId = 0\n",
    "        NPs = []\n",
    "        pos = []\n",
    "        tokens = []\n",
    "        for s in sent.noun_chunks:\n",
    "            NPs.append(s)\n",
    "            \n",
    "        # get pos tag    \n",
    "        for token in doc:\n",
    "            tokens.append(token.text)\n",
    "            pos.append(token.tag_)\n",
    "        \n",
    "        entityMentions = []\n",
    "        # For each quality phrase, check if it's NP\n",
    "        for p in phrases:\n",
    "            for np in NPs:\n",
    "                # find if p is a substring of np\n",
    "                if np.text.find(p) != -1:\n",
    "#                     offset = find(np.text.split(\" \"), p.split(\" \"))\n",
    "                    \n",
    "                    tmp = nlp(p)\n",
    "                    p_tokens = [tok.text for tok in tmp]\n",
    "            \n",
    "                    offset = find(tokens[], p_tokens)\n",
    "                    if offset == -1:\n",
    "                        print(\"ERROR!\", tokens[np.start:np.end], \"->\", p)\n",
    "                        continue\n",
    "                        \n",
    "                    start_offset = np.start + offset - sent.start\n",
    "                    \n",
    "                    ent = {\n",
    "                        \"text\": p,\n",
    "                        \"start\": start_offset,\n",
    "                        \"end\": start_offset + len(p.split(\" \")) - 1,\n",
    "                        \"type\": \"phrase\"\n",
    "                    }\n",
    "                    entityMentions.append(ent)\n",
    "                    \n",
    "                    print(\"##\", \" \".join([tok.text for tok in np]))\n",
    "                    print(np)\n",
    "                    print(np.start, np.end)\n",
    "                    print(p)\n",
    "                    print(ent[\"start\"], ent[\"end\"])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        \n",
    "        res = {\n",
    "            \"articleId\": articleId,\n",
    "            \"sentId\": sentId,\n",
    "            \"tokens\": tokens,\n",
    "            \"pos\": pos,\n",
    "            \"entityMentions\": entityMentions,\n",
    "            \"np_chunks\": [t.text for t in NPs]\n",
    "        }\n",
    "        result.append(res)\n",
    "        \n",
    "        sentId += 1\n",
    "        \n",
    "    return result\n",
    "\n",
    "json.dumps(process_one_doc())\n",
    "# process_one_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concurrent\n",
      "data\n",
      "structure\n"
     ]
    }
   ],
   "source": [
    "tmp = nlp(\"concurrent data structure\")\n",
    "for token in tmp:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find(haystack, needle):\n",
    "    \"\"\"Return the index at which the sequence needle appears in the\n",
    "    sequence haystack, or -1 if it is not found, using the Boyer-\n",
    "    Moore-Horspool algorithm. The elements of needle and haystack must\n",
    "    be hashable.\n",
    "\n",
    "    >>> find([1, 1, 2], [1, 2])\n",
    "    1\n",
    "\n",
    "    \"\"\"\n",
    "    h = len(haystack)\n",
    "    n = len(needle)\n",
    "    skip = {needle[i]: n - i - 1 for i in range(n - 1)}\n",
    "    i = n - 1\n",
    "    while i < h:\n",
    "        for j in range(n):\n",
    "            if haystack[i - j] != needle[-j - 1]:\n",
    "                i += skip.get(haystack[i], n)\n",
    "                break\n",
    "        else:\n",
    "            return i - n + 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find(\"text modern programming languages\".split(\" \"), \"programming language\".split(\" \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
