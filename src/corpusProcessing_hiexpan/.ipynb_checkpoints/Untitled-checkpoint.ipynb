{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences = [sent.string.strip() for sent in doc]\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "from multiprocessing import Lock\n",
    "from collections import deque\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "corpusName = \"test\"\n",
    "FLAGS_POS_TAGGING = 1\n",
    "input_path = \"../../data/\"+corpusName+\"/intermediate/segmentation.txt\"\n",
    "\n",
    "\n",
    "# text = u'The trie <phrase> data structure </phrase> has many properties which make it especially attractive for representing large files of data. These properties include fast retrieval time, quick unsuccessful search determination, and finding the longest match to a given identifier. The main drawback is the space requirement. In this paper the concept of trie compaction is formalized. An exact algorithm for optimal trie compaction and three algorithms for approximate trie compaction are given, and an analysis of the three algorithms is done. The analysis indicate that for actual tries, reductions of around 70 percent in the space required by the uncompacted trie can be expected. The quality of the compaction is shown to be insensitive to the number of nodes, while a more relevant parameter is the alphabet size of the key.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "start_phrase = [{ORTH: u'<phrase>', LEMMA: u'', POS: u'START_PHRASE', TAG: u'START_PHRASE'}]\n",
    "end_phrase = [{ORTH: u'</phrase>', LEMMA: u'', POS: u'END_PHRASE', TAG: u'END_PHRASE'}]\n",
    "\n",
    "nlp.tokenizer.add_special_case(u'<phrase>', start_phrase)\n",
    "nlp.tokenizer.add_special_case(u'</phrase>', end_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    ## add space before and after <phrase> tags\n",
    "    text = re.sub(r\"<phrase>\", \" <phrase> \", text)\n",
    "    text = re.sub(r\"</phrase>\", \" </phrase> \", text)\n",
    "    # text = re.sub(r\"<phrase>\", \" \", text)\n",
    "    # text = re.sub(r\"</phrase>\", \" \", text)\n",
    "    ## add space before and after special characters\n",
    "    text = re.sub(r\"([.,!:?()])\", r\" \\1 \", text)\n",
    "    ## replace multiple continuous whitespace with a single one\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def find(haystack, needle):\n",
    "    \"\"\"Return the index at which the sequence needle appears in the\n",
    "    sequence haystack, or -1 if it is not found, using the Boyer-\n",
    "    Moore-Horspool algorithm. The elements of needle and haystack must\n",
    "    be hashable.\n",
    "\n",
    "    >>> find([1, 1, 2], [1, 2])\n",
    "    1\n",
    "\n",
    "    \"\"\"\n",
    "    h = len(haystack)\n",
    "    n = len(needle)\n",
    "    skip = {needle[i]: n - i - 1 for i in range(n - 1)}\n",
    "    i = n - 1\n",
    "    while i < h:\n",
    "        for j in range(n):\n",
    "            if haystack[i - j] != needle[-j - 1]:\n",
    "                i += skip.get(haystack[i], n)\n",
    "                break\n",
    "        else:\n",
    "            return i - n + 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a three-layer feedforward network\n",
      "2 8\n",
      "feedforward network\n",
      "6 7\n",
      "just two hidden units\n",
      "23 27\n",
      "hidden units\n",
      "25 26\n",
      "transfer functions\n",
      "37 39\n",
      "transfer functions\n",
      "9 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"articleId\": 0, \"sentId\": 0, \"tokens\": [\"We\", \"construct\", \"a\", \"three\", \"-\", \"layer\", \"feedforward\", \"network\", \"(\", \"one\", \"hidden\", \"layer\", \")\", \"which\", \"can\", \"solve\", \"the\", \"N\", \"-\", \"bit\", \"parity\", \"problem\", \"employing\", \"just\", \"two\", \"hidden\", \"units\", \".\", \"We\", \"discuss\", \"the\", \"implications\", \"of\", \"employing\", \"problem\", \"constraints\", \"into\", \"transfer\", \"functions\", \"for\", \"general\", \"pattern\", \"classification\", \"problems\", \".\"], \"pos\": [\"PRP\", \"VBP\", \"DT\", \"CD\", \"HYPH\", \"NN\", \"NN\", \"NN\", \"-LRB-\", \"CD\", \"JJ\", \"NN\", \"-RRB-\", \"WDT\", \"MD\", \"VB\", \"DT\", \"NN\", \"HYPH\", \"NN\", \"NN\", \"NN\", \"VBG\", \"RB\", \"CD\", \"VBN\", \"NNS\", \".\", \"PRP\", \"VBP\", \"DT\", \"NNS\", \"IN\", \"VBG\", \"NN\", \"NNS\", \"IN\", \"NN\", \"NNS\", \"IN\", \"JJ\", \"NN\", \"NN\", \"NNS\", \".\"], \"entityMentions\": [{\"text\": \"feedforward network\", \"start\": 6, \"end\": 7, \"type\": \"phrase\"}, {\"text\": \"hidden units\", \"start\": 25, \"end\": 26, \"type\": \"phrase\"}]}, {\"articleId\": 0, \"sentId\": 0, \"tokens\": [\"We\", \"construct\", \"a\", \"three\", \"-\", \"layer\", \"feedforward\", \"network\", \"(\", \"one\", \"hidden\", \"layer\", \")\", \"which\", \"can\", \"solve\", \"the\", \"N\", \"-\", \"bit\", \"parity\", \"problem\", \"employing\", \"just\", \"two\", \"hidden\", \"units\", \".\", \"We\", \"discuss\", \"the\", \"implications\", \"of\", \"employing\", \"problem\", \"constraints\", \"into\", \"transfer\", \"functions\", \"for\", \"general\", \"pattern\", \"classification\", \"problems\", \".\"], \"pos\": [\"PRP\", \"VBP\", \"DT\", \"CD\", \"HYPH\", \"NN\", \"NN\", \"NN\", \"-LRB-\", \"CD\", \"JJ\", \"NN\", \"-RRB-\", \"WDT\", \"MD\", \"VB\", \"DT\", \"NN\", \"HYPH\", \"NN\", \"NN\", \"NN\", \"VBG\", \"RB\", \"CD\", \"VBN\", \"NNS\", \".\", \"PRP\", \"VBP\", \"DT\", \"NNS\", \"IN\", \"VBG\", \"NN\", \"NNS\", \"IN\", \"NN\", \"NNS\", \"IN\", \"JJ\", \"NN\", \"NN\", \"NNS\", \".\"], \"entityMentions\": [{\"text\": \"transfer functions\", \"start\": 9, \"end\": 10, \"type\": \"phrase\"}]}]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tt = \"A direct and unified approach is used to analyze the efficiency of batched searching of sequential and <phrase>tree-structured</phrase> files. The analysis is applicable to arbitrary search distributions, and closed-form expressions are obtained for the expected batched searching cost and savings. In particular, we consider a search distribution satisfying <phrase>Zipf's law</phrase> for sequential files and four types of uniform (random) search distribution for sequential and <phrase>tree-structured</phrase> files. These results unify and extend earlier research on batched searching and estimating block accesses for <phrase>database systems</phrase>.\"\n",
    "tt = 'We construct a three-layer <phrase>feedforward network</phrase> (one hidden layer) which can solve the N-bit parity problem employing just two <phrase>hidden units</phrase>. We discuss the implications of employing problem constraints into <phrase>transfer functions</phrase> for general pattern classification problems.'\n",
    "\n",
    "def process_one_doc():\n",
    "    article = clean_text(tt)\n",
    "    articleId = 0\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    phrases = []\n",
    "    output_token_list = []\n",
    "    \n",
    "    # go over once \n",
    "    q = deque()\n",
    "    IN_PHRASE_FLAG = False\n",
    "    for token in article.split(\" \"):\n",
    "        if token == \"<phrase>\":\n",
    "            IN_PHRASE_FLAG = True            \n",
    "        elif token == \"</phrase>\":\n",
    "            current_phrase_list = []\n",
    "            while (len(q) != 0):\n",
    "                current_phrase_list.append(q.popleft())\n",
    "            phrases.append(\" \".join(current_phrase_list).lower())\n",
    "            IN_PHRASE_FLAG = False\n",
    "        else:\n",
    "            if IN_PHRASE_FLAG: # in the middle of a phrase, push the (word, pos) tuple\n",
    "                q.append(token)\n",
    "\n",
    "            ## put all the token information into the output fields\n",
    "            output_token_list.append(token)\n",
    "            \n",
    "    text = \" \".join(output_token_list)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "#         print(sent.start, sent.end)\n",
    "        sentId = 0\n",
    "        NPs = []\n",
    "        pos = []\n",
    "        tokens = []\n",
    "        for s in sent.noun_chunks:\n",
    "            NPs.append(s)\n",
    "            \n",
    "        # get pos tag    \n",
    "        for token in doc:\n",
    "            tokens.append(token.text)\n",
    "            pos.append(token.tag_)\n",
    "        \n",
    "        entityMentions = []\n",
    "        # For each quality phrase, check if it's NP\n",
    "        for p in phrases:\n",
    "            for np in NPs:\n",
    "                # find if p is a substring of np\n",
    "                if np.text.find(p) != -1:\n",
    "                    offset = find(tokens[np.start:np.end], p.split(\" \"))\n",
    "                    start_offset = np.start + offset - sent.start\n",
    "                    \n",
    "                    ent = {\n",
    "                        \"text\": p,\n",
    "                        \"start\": start_offset,\n",
    "                        \"end\": start_offset + len(p.split(\" \")) - 1,\n",
    "                        \"type\": \"phrase\"\n",
    "                    }\n",
    "                    entityMentions.append(ent)\n",
    "                     \n",
    "        res = {\n",
    "            \"articleId\": articleId,\n",
    "            \"sentId\": sentId,\n",
    "            \"tokens\": tokens,\n",
    "            \"pos\": pos,\n",
    "            \"entityMentions\": entityMentions,\n",
    "            \"np_chunks\": NPs\n",
    "        }\n",
    "        result.append(res)\n",
    "        \n",
    "        sentId += 1\n",
    "        \n",
    "    return result\n",
    "\n",
    "json.dumps(process_one_doc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
