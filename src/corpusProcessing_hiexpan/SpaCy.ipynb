{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "from multiprocessing import Lock\n",
    "from collections import deque\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "corpusName = \"dblpv2\"\n",
    "FLAGS_POS_TAGGING = 1\n",
    "input_path = \"../../data/\"+corpusName+\"/intermediate/segmentation.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "start_phrase = [\n",
    "    {ORTH: u'<phrase>', LEMMA: u'', POS: u'X', TAG: u'START_PHRASE'}, \n",
    "]\n",
    "end_phrase =     [{ORTH: u'</phrase>', LEMMA: u'', POS: u'X', TAG: u'END_PHRASE'} ]\n",
    "\n",
    "nlp.tokenizer.add_special_case(u'<phrase>', start_phrase)\n",
    "nlp.tokenizer.add_special_case(u'</phrase>', end_phrase)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = u'The trie data-structure has many properties which make it especially attractive for representing large files of data. These properties include fast retrieval time, quick unsuccessful search determination, and finding the longest match to a given identifier. The main drawback is the space requirement. In this paper the concept of trie compaction is formalized. An exact algorithm for optimal trie compaction and three algorithms for approximate trie compaction are given, and an analysis of the three algorithms is done. The analysis indicate that for actual tries, reductions of around 70 percent in the space required by the uncompacted trie can be expected. The quality of the compaction is shown to be insensitive to the number of nodes, while a more relevant parameter is the alphabet size of the key.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trie data-structure has many properties which make it especially attractive for representing large files of data . These properties include fast retrieval time , quick unsuccessful search determination , and finding the longest match to a given identifier . The main drawback is the space requirement . In this paper the concept of trie compaction is formalized . An exact algorithm for optimal trie compaction and three algorithms for approximate trie compaction are given , and an analysis of the three algorithms is done . The analysis indicate that for actual tries , reductions of around 70 percent in the space required by the uncompacted trie can be expected . The quality of the compaction is shown to be insensitive to the number of nodes , while a more relevant parameter is the alphabet size of the key . \n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "## add space before and after <phrase> tags\n",
    "text = re.sub(r\"<phrase>\", \" <phrase> \", text)\n",
    "text = re.sub(r\"</phrase>\", \" </phrase> \", text)\n",
    "# text = re.sub(r\"<phrase>\", \" \", text)\n",
    "# text = re.sub(r\"</phrase>\", \" \", text)\n",
    "## add space before and after special characters\n",
    "text = re.sub(r\"([.,!:?()])\", r\" \\1 \", text)\n",
    "## replace multiple continuous whitespace with a single one\n",
    "text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "print(text)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET DT\n",
      "trie trie NOUN NN\n",
      "data data NOUN NN\n",
      "- - PUNCT HYPH\n",
      "structure structure NOUN NN\n",
      "has have VERB VBZ\n",
      "many many ADJ JJ\n",
      "properties property NOUN NNS\n",
      "which which ADJ WDT\n",
      "make make VERB VBP\n",
      "it -PRON- PRON PRP\n",
      "especially especially ADV RB\n",
      "attractive attractive ADJ JJ\n",
      "for for ADP IN\n",
      "representing represent VERB VBG\n",
      "large large ADJ JJ\n",
      "files file NOUN NNS\n",
      "of of ADP IN\n",
      "data datum NOUN NNS\n",
      ". . PUNCT .\n",
      "These these DET DT\n",
      "properties property NOUN NNS\n",
      "include include VERB VBP\n",
      "fast fast ADJ JJ\n",
      "retrieval retrieval NOUN NN\n",
      "time time NOUN NN\n",
      ", , PUNCT ,\n",
      "quick quick ADJ JJ\n",
      "unsuccessful unsuccessful ADJ JJ\n",
      "search search NOUN NN\n",
      "determination determination NOUN NN\n",
      ", , PUNCT ,\n",
      "and and CCONJ CC\n",
      "finding find VERB VBG\n",
      "the the DET DT\n",
      "longest long ADJ JJS\n",
      "match match NOUN NN\n",
      "to to ADP IN\n",
      "a a DET DT\n",
      "given give VERB VBN\n",
      "identifier identifier ADJ JJ\n",
      ". . PUNCT .\n",
      "The the DET DT\n",
      "main main ADJ JJ\n",
      "drawback drawback NOUN NN\n",
      "is be VERB VBZ\n",
      "the the DET DT\n",
      "space space NOUN NN\n",
      "requirement requirement NOUN NN\n",
      ". . PUNCT .\n",
      "In in ADP IN\n",
      "this this DET DT\n",
      "paper paper NOUN NN\n",
      "the the DET DT\n",
      "concept concept NOUN NN\n",
      "of of ADP IN\n",
      "trie trie NOUN NN\n",
      "compaction compaction NOUN NN\n",
      "is be VERB VBZ\n",
      "formalized formalize VERB VBN\n",
      ". . PUNCT .\n",
      "An an DET DT\n",
      "exact exact ADJ JJ\n",
      "algorithm algorithm NOUN NN\n",
      "for for ADP IN\n",
      "optimal optimal ADJ JJ\n",
      "trie trie NOUN NN\n",
      "compaction compaction NOUN NN\n",
      "and and CCONJ CC\n",
      "three three NUM CD\n",
      "algorithms algorithm NOUN NNS\n",
      "for for ADP IN\n",
      "approximate approximate ADJ JJ\n",
      "trie trie NOUN NN\n",
      "compaction compaction NOUN NN\n",
      "are be VERB VBP\n",
      "given give VERB VBN\n",
      ", , PUNCT ,\n",
      "and and CCONJ CC\n",
      "an an DET DT\n",
      "analysis analysis NOUN NN\n",
      "of of ADP IN\n",
      "the the DET DT\n",
      "three three NUM CD\n",
      "algorithms algorithm NOUN NNS\n",
      "is be VERB VBZ\n",
      "done do VERB VBN\n",
      ". . PUNCT .\n",
      "The the DET DT\n",
      "analysis analysis NOUN NN\n",
      "indicate indicate VERB VBP\n",
      "that that ADP IN\n",
      "for for ADP IN\n",
      "actual actual ADJ JJ\n",
      "tries try NOUN NNS\n",
      ", , PUNCT ,\n",
      "reductions reduction NOUN NNS\n",
      "of of ADP IN\n",
      "around around ADV RB\n",
      "70 70 NUM CD\n",
      "percent percent NOUN NN\n",
      "in in ADP IN\n",
      "the the DET DT\n",
      "space space NOUN NN\n",
      "required require VERB VBN\n",
      "by by ADP IN\n",
      "the the DET DT\n",
      "uncompacted uncompacted ADJ JJ\n",
      "trie trie NOUN NN\n",
      "can can VERB MD\n",
      "be be VERB VB\n",
      "expected expect VERB VBN\n",
      ". . PUNCT .\n",
      "The the DET DT\n",
      "quality quality NOUN NN\n",
      "of of ADP IN\n",
      "the the DET DT\n",
      "compaction compaction NOUN NN\n",
      "is be VERB VBZ\n",
      "shown show VERB VBN\n",
      "to to PART TO\n",
      "be be VERB VB\n",
      "insensitive insensitive ADJ JJ\n",
      "to to ADP IN\n",
      "the the DET DT\n",
      "number number NOUN NN\n",
      "of of ADP IN\n",
      "nodes node NOUN NNS\n",
      ", , PUNCT ,\n",
      "while while ADP IN\n",
      "a a DET DT\n",
      "more more ADV RBR\n",
      "relevant relevant ADJ JJ\n",
      "parameter parameter NOUN NN\n",
      "is be VERB VBZ\n",
      "the the DET DT\n",
      "alphabet alphabet ADJ JJ\n",
      "size size NOUN NN\n",
      "of of ADP IN\n",
      "the the DET DT\n",
      "key key NOUN NN\n",
      ". . PUNCT .\n",
      "=================\n",
      "The trie data-structure 0 5\n",
      "many properties 6 8\n",
      "it 10 11\n",
      "large files 15 17\n",
      "data 18 19\n",
      "These properties 20 22\n",
      "fast retrieval time 23 26\n",
      "quick unsuccessful search determination 27 31\n",
      "the longest match 34 37\n",
      "The main drawback 42 45\n",
      "the space requirement 46 49\n",
      "this paper 51 53\n",
      "the concept 53 55\n",
      "trie compaction 56 58\n",
      "An exact algorithm 61 64\n",
      "optimal trie compaction 65 68\n",
      "three algorithms 69 71\n",
      "approximate trie compaction 72 75\n",
      "an analysis 79 81\n",
      "the three algorithms 82 85\n",
      "The analysis 88 90\n",
      "actual tries 93 95\n",
      "reductions 96 97\n",
      "around 70 percent 98 101\n",
      "the space 102 104\n",
      "the uncompacted trie 106 109\n",
      "The quality 113 115\n",
      "the compaction 116 118\n",
      "the number 124 126\n",
      "nodes 127 128\n",
      "a more relevant parameter 130 134\n",
      "the alphabet size 135 138\n",
      "the key 139 141\n"
     ]
    }
   ],
   "source": [
    "# print(doc)\n",
    "\n",
    "# for s in doc.sents:\n",
    "#     print(s)\n",
    "\n",
    "for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#           token.shape_, token.is_alpha, token.is_stop)\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)\n",
    "    \n",
    "print(\"=================\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text,chunk.start,chunk.end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many properties\n",
      "ADJ\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "many"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [ele for ele in doc.noun_chunks]\n",
    "a = test[1]\n",
    "print(a)\n",
    "print(doc[a.start].pos_)\n",
    "print(a.end)\n",
    "\n",
    "doc[a.start].pos_ in [\"DT\"]\n",
    "doc[a.start] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__repr__', '__hash__', '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__', '__iter__', '__len__', '__getitem__', '__new__', 'set_extension', 'get_extension', 'has_extension', 'as_doc', 'merge', 'similarity', 'get_lca_matrix', 'to_array', '_recalculate_indices', '_', 'vocab', 'sent', 'has_vector', 'vector', 'vector_norm', 'sentiment', 'text', 'text_with_ws', 'noun_chunks', 'root', 'lefts', 'rights', 'n_lefts', 'n_rights', 'subtree', 'ent_id', 'ent_id_', 'orth_', 'lemma_', 'upper_', 'lower_', 'string', 'label_', 'doc', 'start', 'end', 'start_char', 'end_char', 'label', '_vector', '_vector_norm', '__doc__', '__pyx_vtable__', '__reduce__', '__setstate__', '__str__', '__getattribute__', '__setattr__', '__delattr__', '__init__', '__reduce_ex__', '__subclasshook__', '__init_subclass__', '__format__', '__sizeof__', '__dir__', '__class__']\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(a.__dir__())\n",
    "print(a.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoPhraseOutput(object):\n",
    "    def __init__(self, input_path, nlp):\n",
    "        self.input_path = input_path\n",
    "        self.nlp = nlp\n",
    "        self.phrase_to_pos_sequence = {}  # key: lower case phrase, value: a dict of {\"pos_sequence\": count}\n",
    "        self.pos_sequence_to_score = {}  # key: a pos sequence, value: a score in [0,0, 1.0]\n",
    "        self.candidate_phrase = []\n",
    "\n",
    "    def parse_one_doc(self, doc):\n",
    "        \"\"\" Parse each document, update the phrase_to_pos_sequence, and convert the json format from Stanford\n",
    "        CoreNLP to Ellen's sentences.json.raw format\n",
    "\n",
    "        :param doc:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ## replace non-ascii character\n",
    "        doc = re.sub(r'[^\\x00-\\x7F]+', ' ', doc)\n",
    "        ## add space before and after <phrase> tags\n",
    "        doc = re.sub(r\"<phrase>\", \" <phrase> \", doc)\n",
    "        doc = re.sub(r\"</phrase>\", \" </phrase> \", doc)\n",
    "        ## add space before and after special characters\n",
    "        doc = re.sub(r\"([.,!:?()])\", r\" \\1 \", doc)\n",
    "        ## replace multiple continuous whitespace with a single one\n",
    "        doc = re.sub(r\"\\s{2,}\", \" \", doc)\n",
    "\n",
    "        res = self.nlp.annotate(doc, properties={\n",
    "            \"annotators\": \"tokenize,ssplit,pos\",\n",
    "            \"outputFormat\": \"json\"\n",
    "        })\n",
    "        output_sents = []\n",
    "        for sent in res['sentences']:\n",
    "            ## a new sentence\n",
    "            output_token_list = []\n",
    "            output_pos_list = []\n",
    "\n",
    "            IN_PHRASE_FLAG = False\n",
    "            q = deque()\n",
    "            for token in sent['tokens']:\n",
    "                word = token['word']\n",
    "                pos = token['pos']\n",
    "                if word == \"<phrase>\": # the start of a phrase\n",
    "                    IN_PHRASE_FLAG = True\n",
    "                    ## Mark the position of a phrase for postprecessing\n",
    "                    output_token_list.append(word)\n",
    "                    output_pos_list.append(\"START_PHRASE\")\n",
    "                elif word == \"</phrase>\": # the end of a phrase\n",
    "                    ## obtain the information of current phrase\n",
    "                    current_phrase_list = []\n",
    "                    while (len(q) != 0):\n",
    "                        current_phrase_list.append(q.popleft())\n",
    "                    phrase = \" \".join([ele[0] for ele in current_phrase_list]).lower() # convert to lower case\n",
    "                    pos_sequence = \" \".join([ele[1] for ele in current_phrase_list])\n",
    "\n",
    "                    ## update phrase information\n",
    "                    if phrase not in self.phrase_to_pos_sequence:\n",
    "                        self.phrase_to_pos_sequence[phrase] = {}\n",
    "\n",
    "                    if pos_sequence not in self.phrase_to_pos_sequence[phrase]:\n",
    "                        self.phrase_to_pos_sequence[phrase][pos_sequence] = 1\n",
    "                    else:\n",
    "                        self.phrase_to_pos_sequence[phrase][pos_sequence] += 1\n",
    "\n",
    "                    IN_PHRASE_FLAG = False\n",
    "\n",
    "                    ## Mark the position of a phrase for postprecessing\n",
    "                    output_token_list.append(word)\n",
    "                    output_pos_list.append(\"END_PHRASE\")\n",
    "                else:\n",
    "                    if IN_PHRASE_FLAG: # in the middle of a phrase, push the (word, pos) tuple\n",
    "                        q.append((word,pos))\n",
    "\n",
    "                    ## put all the token information into the output fields\n",
    "                    output_token_list.append(word)\n",
    "                    output_pos_list.append(pos)\n",
    "\n",
    "            ## Finish processing one sentence, add the result into output_sents\n",
    "            output_sents.append({\n",
    "                \"tokens\": output_token_list,\n",
    "                \"pos\": output_pos_list\n",
    "            })\n",
    "\n",
    "        if (len(q) != 0):\n",
    "            print(\"[ERROR]: mismatched </phrase> in document: %s\" % doc)\n",
    "\n",
    "        return output_sents\n",
    "\n",
    "    def save_phrase_to_pos_sequence(self, output_path=\"\"):\n",
    "        with open(output_path, \"w\") as fout:\n",
    "            for phrase in self.phrase_to_pos_sequence:\n",
    "                fout.write(phrase)\n",
    "                fout.write(\"\\t\")\n",
    "                fout.write(str(self.phrase_to_pos_sequence[phrase]))\n",
    "                fout.write(\"\\n\")\n",
    "\n",
    "    def load_phrase_to_pos_sequence(self, input_path=\"\"):\n",
    "        with open(input_path, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                seg = line.split(\"\\t\")\n",
    "                if len(seg) < 2:\n",
    "                    continue\n",
    "                phrase = seg[0]\n",
    "                pos_sequence = eval(seg[1])\n",
    "                self.phrase_to_pos_sequence[phrase] = pos_sequence\n",
    "\n",
    "    def score_pos_sequene(self):\n",
    "        for pos_sequence_list in self.phrase_to_pos_sequence.values():\n",
    "            for pos_sequence in pos_sequence_list:\n",
    "                if pos_sequence not in self.pos_sequence_to_score:\n",
    "                    if \"NN\" not in pos_sequence:\n",
    "                        self.pos_sequence_to_score[pos_sequence] = 0.0\n",
    "                    else:\n",
    "                        self.pos_sequence_to_score[pos_sequence] = 1.0\n",
    "\n",
    "    def obtain_candidate_phrase(self, threshold = 0.8, min_sup = 5):\n",
    "        print(\"Number of phrases before filtering = %s\" % len(self.phrase_to_pos_sequence))\n",
    "        for phrase in self.phrase_to_pos_sequence:\n",
    "            phrase_score = 0\n",
    "            freq = sum(self.phrase_to_pos_sequence[phrase].values())\n",
    "            if freq < min_sup:\n",
    "                continue\n",
    "            for pos_sequence in self.phrase_to_pos_sequence[phrase].keys():\n",
    "                pos_sequence_weight = float(self.phrase_to_pos_sequence[phrase][pos_sequence]) / freq\n",
    "                pos_sequence_score = self.pos_sequence_to_score[pos_sequence]\n",
    "                phrase_score += (pos_sequence_weight * pos_sequence_score)\n",
    "            if phrase_score >= threshold:\n",
    "                self.candidate_phrase.append(phrase)\n",
    "        print(\"Number of phrases after filtering = %s\" % len(self.candidate_phrase))\n",
    "\n",
    "    def save_candidate_phrase(self, output_path=\"\"):\n",
    "        with open(output_path, \"w\") as fout:\n",
    "            for phrase in self.candidate_phrase:\n",
    "                fout.write(phrase+\"\\n\")\n",
    "                \n",
    "                \n",
    "autoPhraseOutput = AutoPhraseOutput(input_path=input_path, nlp=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
